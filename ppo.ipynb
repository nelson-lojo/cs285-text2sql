{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab906c04-d4b0-4140-be32-554f72c666ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2023-12-14 02:52:12.632176: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-14 02:52:12.683426: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Tuple, Literal\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, PeftConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import (\n",
    "    PPOTrainer,\n",
    "    PPOConfig,\n",
    "    AutoModelForCausalLMWithValueHead\n",
    ")\n",
    "\n",
    "from reward import get_reward\n",
    "\n",
    "SFT_ADAPTER_DIRECTORY = \"./open_llama_3b_v2_sft/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f557a80b-3930-48f9-ac5e-11ef3c5901f5",
   "metadata": {},
   "source": [
    "## Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b08806b1-926e-446b-be6e-28cf98b43361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model from Hugging Face hub\n",
    "base_model = \"openlm-research/open_llama_3b_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "443a0896-d4e0-4e91-80c9-e1103ab8aadd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b35b3e9-f616-43fb-a6c4-5e61ba241f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model and quantization configs\n",
    "config = AutoConfig.from_pretrained(base_model, trust_remote_code=True)\n",
    "config.init_device = 'cuda:0' # For fast initialization directly on GPU!\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    # torch_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6da0def-8497-4a53-9e15-66f2db9ef112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:229: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load SFT model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "model_with_value_head = AutoModelForCausalLMWithValueHead(model)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model_with_value_head, SFT_ADAPTER_DIRECTORY)\n",
    "peft_model = peft_model.merge_and_unload(safe_merge=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f135491-77bb-40b0-b33b-b394dcd417a0",
   "metadata": {},
   "source": [
    "## Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae23c705-3ae8-45b0-a2d2-d993e3c7deef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = \"samlhuillier/sql-create-context-spider-intersect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbfc0444-07fe-486d-862a-ff42de93321d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_template = \"\\n-- Answer:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd9f394-fcc6-4f2c-8173-09ce4d01d47f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_prompt(example) -> dict:\n",
    "    # Combine 'context' and 'question' into a single string called 'query'\n",
    "    query = f\"{example['context']} \\n-- Question: {example['question']}{response_template}\"\n",
    "    # Return a dictionary with 'query' and 'answer'\n",
    "    return {\"query\": query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8036d3fe-06aa-4281-b9bc-8cac9ebf9d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # 450 is a little over 400, which is the max length query (147) + max length answer (253)\n",
    "    return tokenizer(examples['query'], padding=\"max_length\", truncation=True, max_length=150) # the longest query is 147 tokens long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77260b0b-cde9-4fb0-b28f-4ba9fc66ebbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7365c24-794b-46ce-aa7a-f3d761fd9c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c507aa203a54a9da338004f54bcb55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3961 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = dataset['train'].map(format_prompt)\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "996070b6-6cb3-4368-bae6-5f7913557919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee42c0e0bba84e9da7405837bb5dd7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/568 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_dataset = dataset['validation'].map(format_prompt)\n",
    "tokenized_valid_dataset = valid_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0712572-fa7a-4b9b-b8dc-1755c8c8b7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['answer', 'question', 'context', 'db_id', 'query', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 3961\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f926caf-29fb-430b-b910-a0075087c483",
   "metadata": {},
   "source": [
    "## Trainer Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65b096fc-e0bd-454c-ae03-a15c92f713ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "train_dataloader = DataLoader(tokenized_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "trainer_config = PPOConfig(\n",
    "    model_name=base_model,\n",
    "    # query_dataset=tokenized_train_dataset,\n",
    "    log_with=\"tensorboard\",  # Specify that you are using TensorBoard for logging\n",
    "    project_kwargs={\"logging_dir\": \"./ppo_results\"},\n",
    "    \n",
    "    ppo_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    batch_size=batch_size,\n",
    "    # mini_batch_size=script_args.mini_batch_size, # Default: 1\n",
    "    # steps=script_args.steps, # default: 20_000\n",
    "    \n",
    "    # optimize_cuda_cache=True,\n",
    "    is_peft_model=True\n",
    "    # seed=script_args.seed, # Default: 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a86c7c4-09ef-4ef0-a046-7c43b79816c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(\n",
    "    model=model_with_value_head,\n",
    "    dataset=tokenized_train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    config=trainer_config,\n",
    "    optimizer=\"paged_adamw_32bit\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e8449a-15ad-4968-a936-50169b05eb6a",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8a132cf-f25c-4f31-b058-db07e58013e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253, 147)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_num_tokens(text):\n",
    "    return len(tokenizer(text)[\"input_ids\"])\n",
    "\n",
    "max(map(get_num_tokens, tokenized_train_dataset[\"answer\"])), max(map(get_num_tokens, tokenized_train_dataset[\"query\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71cacd51-7b77-4a4c-960c-3b8356ce80a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_tokens_for_step(toks):\n",
    "    return list(map(torch.squeeze, torch.split(torch.vstack(toks).T, split_size_or_sections=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54a02bde-6a0c-4f43-8d6b-5fe1678d7b01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response for batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "0it [00:02, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response generated for batch 0\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '<s>id>', 'CRE VMC', 'ATEARCH,', 'TABLEARPT', 'Sports)\\n', 'info 3', '(\\nD', 's--Sc', 'port Question?', 'name:active', 'V findr', 'ARCH the dr', 'AR name\\n', ', and the', 'on age is', 'sch of the', 'olar the attack', 'ship pilot program', 'V who may', 'ARCH has little', 'AR won in', ') the infamous', ' most consistent', '\\n number bonus', '-- of an', 'Question times Maple', ': among those', 'Which the VA', 'sport pilots will', 'has who you', 'most are happy', 'number younger than', 'of than can', 'students 2', 'on39', 'scholarship0.', '?.var', '\\n\\n\\n', '---- IC', 'Answer Answerternal', ':: bad', '\\n\\nSince']\n",
      "achieved rewards:\n",
      "[tensor(0., dtype=torch.float64), tensor(0., dtype=torch.float64)]\n",
      "150\n",
      "Query Lens: [150, 150]\n",
      "[9, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "vstack(): argument 'tensors' (position 1) must be tuple of Tensors, not BatchEncoding",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_181117/1957480469.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     )))\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mrt_for_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix_tokens_for_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Resp Lens:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrt_for_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m## Perform PPO step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_181117/3981975419.py\u001b[0m in \u001b[0;36mfix_tokens_for_step\u001b[0;34m(toks)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfix_tokens_for_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_size_or_sections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: vstack(): argument 'tensors' (position 1) must be tuple of Tensors, not BatchEncoding"
     ]
    }
   ],
   "source": [
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 1,  # Limit the maximum length to ensure faster generation\n",
    "    \"top_k\": 0.,  # Don't use Top-K sampling for performance (and reduce randomness) \n",
    "    \"top_p\": 1.0,  # Don't use nucleus sampling with a reasonable threshold\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "}\n",
    "\n",
    "model.is_peft_model = True\n",
    "for epoch, batch in tqdm(enumerate(train_dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "    # print(query_tensors)  # Debug print\n",
    "\n",
    "    ## Generate response (action)\n",
    "    print(f\"Generating response for batch {epoch}\")  # Debug 3   \n",
    "    response_tensors = trainer.generate(query_tensors, **generation_kwargs)\n",
    "    print(f\"Response generated for batch {epoch}\")  # Debug 4\n",
    "\n",
    "    resps = tokenizer.batch_decode(map(lambda resp: resp.squeeze(), response_tensors))\n",
    "    batch[\"response\"] = resps\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    response_tensors = tokenizer(resps, padding=\"max_length\", max_length=256)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    print(resps)\n",
    "\n",
    "    ## Compute reward (see paper)\n",
    "    rewards = [ \n",
    "        torch.tensor(get_reward(db, response, ans)) \n",
    "        for response, db, ans in zip(\n",
    "            batch[\"response\"], batch[\"db_id\"], batch[\"answer\"]\n",
    "        )\n",
    "    ]\n",
    "    print(f\"achieved rewards:\\n{rewards}\")\n",
    "    print(f\"{len(query_tensors)}\")\n",
    "#     # rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "    qt_for_logs = fix_tokens_for_step(query_tensors)\n",
    "    print(\"Query Lens:\", list(map(len, qt_for_logs)))\n",
    "    print(list(map(\n",
    "        len, response_tensors\n",
    "    )))\n",
    "    rt_for_logs = fix_tokens_for_step(response_tensors)\n",
    "    print(\"Resp Lens:\", list(map(len, rt_for_logs)))\n",
    "    ## Perform PPO step\n",
    "    stats = trainer.step(qt_for_logs, rt_for_logs, rewards)\n",
    "    trainer.log_stats(stats, batch, rewards)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55359be-14e3-4187-9887-fe1315d8b531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Save model\n",
    "trainer.save_model(\"open_llama_3b_v2_sft_plus_ppo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb1049-5630-4fde-8f95-7d4717ac6a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "logging_dir = \"ppo_results/runs\"\n",
    "notebook.start(\"--logdir {} --port 4000\".format(logging_dir))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
