{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cfb0025-ba9b-4067-b980-d77db566c557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2023-12-13 03:17:18.778257: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-13 03:17:18.828290: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from typing import Union, Literal, Tuple\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import (\n",
    "    DPOTrainer, \n",
    "    DataCollatorForCompletionOnlyLM\n",
    ")\n",
    "\n",
    "SFT_ADAPTER_DIRECTORY = \"./open_llama_3b_v2_sft/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e0696-a01a-47ad-8df9-615ef6b14a0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9122e097-89ac-4ca2-b90e-6eb2e146c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model from Hugging Face hub\n",
    "base_model = \"openlm-research/open_llama_3b_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba86e3d9-0a17-445e-9acd-c513ee70c646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af43a7ea-602c-4322-8038-2b652a4a5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and quantization configs\n",
    "config = AutoConfig.from_pretrained(base_model, trust_remote_code=True)\n",
    "config.init_device = 'cuda:0' # For fast initialization directly on GPU!\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    # torch_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3413bd97-d301-4ffa-a1b3-010e53c938b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    SFT_ADAPTER_DIRECTORY,\n",
    "    quantization_config=quant_config,\n",
    "    trust_remote_code=True,\n",
    "    is_trainable=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed09fbe4-b176-4280-96b0-d12357a9ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference model\n",
    "model_ref = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    SFT_ADAPTER_DIRECTORY,\n",
    "    quantization_config=quant_config,\n",
    "    trust_remote_code=True,\n",
    "    is_trainable=False,\n",
    ")\n",
    "model_ref.config.use_cache = False\n",
    "model_ref.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693eeb3b-c358-4070-8859-2759fb8cf922",
   "metadata": {},
   "source": [
    "## Dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf5740bd-b0b5-4645-8bb6-8824533aae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"samlhuillier/sql-create-context-spider-intersect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39367609-cba8-4ee0-a32d-dbfb97faf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"\\n-- Answer:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21f425ef-6e0a-4061-a829-0482a292fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens_with_ids(txt):\n",
    "    tokens = tokenizer.tokenize(txt, add_special_tokens=False)\n",
    "    token_ids = tokenizer.encode(txt, add_special_tokens=False)\n",
    "    print(list(zip(tokens, token_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9201b1a0-7ebc-4486-aa73-35096b3de7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('▁', 29500), ('<0x0A>', 13), ('--', 559), ('▁Answer', 13910), (':', 29537), ('<0x0A>', 13)]\n"
     ]
    }
   ],
   "source": [
    "print_tokens_with_ids(response_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ed99aab-ae87-4043-93f6-5c1e1bd82df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('▁--', 1472), ('▁Question', 10706), (':', 29537), ('▁HI', 27003), ('<0x0A>', 13), ('--', 559), ('▁Answer', 13910), (':', 29537), ('<0x0A>', 13)]\n"
     ]
    }
   ],
   "source": [
    "print_tokens_with_ids(f\"-- Question: HI{response_template}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57c7b1d3-d997-485c-86c4-5f9653c87edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(example) -> Tuple[str, str]:\n",
    "    return f\"{example['context']} \\n-- Question: {example['question']}{response_template}\", example['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6c42ff0-bc6f-45b8-a1b8-c1fabf9f2529",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def mutate(response, num_tokens=1):\n",
    "    \"\"\" Change `num_tokens` to a random token in the vocabulary \"\"\"\n",
    "    # Encode the string\n",
    "    tokens = tokenizer.encode(response, add_special_tokens=False)\n",
    "\n",
    "    # Select `num_tokens` mutation indices\n",
    "    if tokenizer.decode(tokens[1:]) == tokenizer.decode(tokens):\n",
    "        # this means an additional prefix token was added\n",
    "        mutation_indices = torch.randperm(len(tokens) - 1) + 1\n",
    "    else:\n",
    "        mutation_indices = torch.randperm(len(tokens))\n",
    "    mutation_indices = mutation_indices[:num_tokens]\n",
    "\n",
    "    # Mutate those indices\n",
    "    for idx in mutation_indices:\n",
    "        tokens[idx] = torch.randint(tokenizer.vocab_size, (1,)).item()\n",
    "    \n",
    "    return tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9500a0da-9674-4035-b4c5-3a356c8bfc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-- Hello, this is a long sentence RodSignature string mutation'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutate(\"-- Hello, this is a long sentence to demonstrate string mutation\", num_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f41a9378-8b13-4782-9db7-7e71127cf5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(split: Literal[\"train\", \"validation\"] = \"train\", toks_to_mutate=1):\n",
    "    \"\"\"Load the dataset from Hugging Face and on-the-fly do (1) convert it to the necessary format and (2) impose token mutations.\n",
    "\n",
    "    The dataset is converted to a dictionary with the following structure:\n",
    "    {\n",
    "        'prompt': List[str],\n",
    "        'chosen': List[str],\n",
    "        'rejected': List[str],\n",
    "    }\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    original_columns = dataset.column_names\n",
    "\n",
    "    tokens_to_mutate = toks_to_mutate if split == \"train\" else 0\n",
    "    \n",
    "    def batched_mutate(examples):\n",
    "        out = {\n",
    "            \"prompt\" : [],\n",
    "            \"chosen\" : [],\n",
    "            \"rejected\" : []\n",
    "        }\n",
    "        for question, ctx, ans in zip(examples[\"question\"], examples[\"context\"], examples[\"answer\"]):\n",
    "            prompt, resp = format_prompt({\n",
    "                \"question\" : question,\n",
    "                \"context\" : ctx,\n",
    "                \"answer\" : ans\n",
    "            })\n",
    "            out[\"prompt\"].append(prompt)\n",
    "            out[\"chosen\"].append(resp)\n",
    "            out[\"rejected\"].append(mutate(resp, num_tokens=tokens_to_mutate))\n",
    "        return out\n",
    "\n",
    "    dataset.set_transform(batched_mutate)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38925ecf-c61e-4eae-83ca-60ca3653b36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['answer', 'question', 'context', 'db_id'],\n",
       "    num_rows: 3961\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = get_dataset(\"train\", toks_to_mutate=5)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5aa180fa-b4b5-4f9c-93b1-440f4def1486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'CREATE TABLE head (age INTEGER) \\n-- Question: How many heads of the departments are older than 56 ?\\n-- Answer:\\n',\n",
       " 'chosen': 'SELECT count(*) FROM head WHERE age  >  56',\n",
       " 'rejected': 'SELECT count offset Ker Dolத WHERE age  > experience56'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6ca7662-ea7a-4a85-91cd-46b3074f7560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'CREATE TABLE head (age INTEGER) \\n-- Question: How many heads of the departments are older than 56 ?\\n-- Answer:\\n',\n",
       " 'chosen': 'SELECT count(*) FROM head WHERE age  >  56',\n",
       " 'rejected': 'SELECT count(*世 FROMicip AP ageading>  5FAULT'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ead7cbf-70f3-4497-b3ab-7ab88f95050d",
   "metadata": {},
   "source": [
    "## Trainer Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dcd7ac6-3507-4605-b7fb-a77ff0141c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:229: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    model_ref, # The model with peft adapters turned off will be used as a reference model if not provided\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds,\n",
    "    beta=0.2, \n",
    "    max_length=2048,\n",
    "    max_prompt_length=1500,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./dpo_results\",\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "\n",
    "        max_grad_norm=0.3,\n",
    "        warmup_ratio=0.03,\n",
    "        \n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=0.001,\n",
    "        num_train_epochs=1,\n",
    "        max_steps=-1,\n",
    "        per_device_train_batch_size=2,\n",
    "        \n",
    "        gradient_accumulation_steps=1,\n",
    "        save_steps=500,\n",
    "        logging_steps=100,\n",
    "        logging_first_step=True,\n",
    "        \n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "\n",
    "        remove_unused_columns=False,\n",
    "        lr_scheduler_type=\"constant\",\n",
    "        report_to=\"tensorboard\"\n",
    "    ),\n",
    "    peft_config=LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b404a79-a657-47c7-ac20-4912025b5053",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8318cb8-7952-467d-ba1e-cf10d41a3824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='1981' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  51/1981 00:47 < 31:29, 1.02 it/s, Epoch 0.03/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.619300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2909164e-d475-469a-8415-8b83ae375444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "new_model = \"open_llama_3b_v2_sft_plus_dpo\"\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b10805-432b-4c93-bcdd-61c05ef32359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
